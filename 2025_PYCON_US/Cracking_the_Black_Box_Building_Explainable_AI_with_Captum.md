**Title:**  
Cracking the Black Box: Building Explainable AI with Captum

**Session Type:**
Talk (30 mins)

**Track:**  
Talk

**Description:**  
Artificial Intelligence often operates as a “black box,” leaving developers and stakeholders unsure how decisions are made. Explainable AI (XAI) addresses this challenge by providing tools to interpret and visualize AI model behavior, helping to build trust and transparency in AI systems. This session will focus on Captum, one of the leading Python libraries that makes black-box models interpretable.

Attendees will explore the fundamentals of XAI, learn how to integrate these libraries into their workflows, and discover techniques for generating feature attributions, decision-path visualizations, and scenario-specific insights. By the end of the session, participants will be equipped to apply XAI techniques to their own projects, gaining actionable insights into model behavior. A live demo will showcase the application of Captum to a trained model, providing clarity on how to debug, explain, and optimize real-world AI systems.

**Notes:**  
This session addresses a critical challenge in AI adoption: trust and transparency. By equipping attendees with practical tools and techniques, it bridges the gap between AI complexity and human interpretability. The inclusion of live demos ensures attendees leave with actionable knowledge to apply XAI principles using Python tools in their projects, making it a valuable addition to the PyCon community.

**Outline:**  
1. Introduction to Explainable AI (5 min)
Importance of interpretability in AI/ML applications.
Overview of Captum and its role in explaining PyTorch models.

2. Key Features of Captum (5 min)
Attribution methods (e.g., Integrated Gradients, Saliency).
Visualizing model decisions and debugging insights.

3. Practical Use Case (10 min)
Walkthrough of implementing Captum on a sample PyTorch model.
Demonstrating attributions and actionable insights with visualizations.

4. Live Demo (5 min)
Real-time exploration of a PyTorch model using Captum.
Explaining predictions for edge cases and anomalies.

5. Q&A (5 min)

**Category:**  
Data Analysis

**Audience Level:**  
Some Experience

**Previously Given Presentation:**  
No

**Release Recording:**  
Yes
